{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69950ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ë¼ì´ë¸ŒëŸ¬ë¦¬, ê²½ë¡œ, ë¡œê±° ì„¤ì • ---\n",
    "\n",
    "# ë°ì´í„° ë¶„ì„ ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os, sys\n",
    "from datetime import datetime                                         \n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# ëª¨ë¸ë§ ë° ê¸°ê³„ í•™ìŠµì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import optuna \n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# --- 1.1. ë¡œê±°(ì‹¤í–‰ ê¸°ë¡ ë¡œê·¸ ì €ì¥) ì„í¬íŠ¸ ---\n",
    "# ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì™¸ë¶€ logger.py ëª¨ë“ˆì„ ì„í¬í•©ë‹ˆë‹¤.\n",
    "try:\n",
    "    src_path = os.path.abspath(os.path.join(os.getcwd(), \"../../src/log\"))\n",
    "    sys.path.insert(0, src_path)\n",
    "    from logger import Logger\n",
    "except ImportError:\n",
    "    print(\"ì˜¤ë¥˜: 'logger.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'src/log' ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    # ê°„ë‹¨í•œ ëŒ€ì²´ ë¡œê±° ì •ì˜\n",
    "    class Logger:\n",
    "        def __init__(self, log_path): print(f\"ëŒ€ì²´ ë¡œê±° í™œì„±í™”. ë¡œê·¸ëŠ” ê¸°ë¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        def write(self, message, **kwargs): print(message)\n",
    "        def start_redirect(self): pass\n",
    "        def close(self): pass\n",
    "\n",
    "\n",
    "# --- 1.2. ê²½ë¡œ ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì‚¬ìš©ì ìš”ì²­ ê¸°ë°˜) ---\n",
    "# í˜„ì¬ ì‹œê°„ ê¸°ì¤€ ë…„ì›”ì¼_ì‹œê° ë¬¸ìì—´ ìƒì„±\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ì‹¤í–‰ ë¡œê·¸ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "LOG_DIR                         = '../../data/logs/price_prediction_8_logs'\n",
    "LOG_FILENAME                    = f\"price_prediction_8_{timestamp}.log\"\n",
    "LOG_PATH                        = os.path.join(LOG_DIR, LOG_FILENAME)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "logger = Logger(log_path=LOG_PATH)\n",
    "\n",
    "# ë°ì´í„° ë° ê²°ê³¼ë¬¼ ê²½ë¡œ ì„¤ì •\n",
    "RAW_DIR                         = '../../data/processed/clean_data'\n",
    "TRAIN_FILENAME                  = 'train.csv'\n",
    "TEST_FILENAME                   = 'test.csv'\n",
    "TRAIN_PATH                      = os.path.join(RAW_DIR, TRAIN_FILENAME)\n",
    "TEST_PATH                       = os.path.join(RAW_DIR, TEST_FILENAME)\n",
    "\n",
    "PARAMS_DIR                      = '../../data/processed/params'\n",
    "PARAMS_FILENAME                 = 'best_params_8.json'\n",
    "PARAMS_PATH                     = os.path.join(PARAMS_DIR, PARAMS_FILENAME)\n",
    "\n",
    "SUBMISSION_DIR                  = '../../data/processed/submissions'\n",
    "SUBMISSION_TEMPLATE_FILENAME    = 'baseline_code_sample_submission.csv'\n",
    "SUBMISSION_FILENAME             = f'price_prediction_8_submission_{timestamp}.csv'\n",
    "SUBMISSION_TEMPLATE_PATH        = os.path.join(SUBMISSION_DIR, SUBMISSION_TEMPLATE_FILENAME)\n",
    "SUBMISSION_PATH                 = os.path.join(SUBMISSION_DIR, SUBMISSION_FILENAME)\n",
    "\n",
    "IMAGE_DIR                       = '../../images/price_prediction_8/1'\n",
    "IMAGE_FILENAME                  = 'price_prediction_8_model.pkl'\n",
    "IMAGE_PATH                      = os.path.join(IMAGE_DIR, IMAGE_FILENAME)\n",
    "\n",
    "MODEL_DIR                       = '../../model/price_prediction_8_{timestamp}'\n",
    "MODEL_FILENAME                  = 'price_prediction_8_model.pkl'\n",
    "MODEL_PATH                      = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(PARAMS_DIR, exist_ok=True)\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "logger.start_redirect()\n",
    "logger.write(\"=\"*60)\n",
    "logger.write(\">> [price_prediction8] ì•„íŒŒíŠ¸ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ë§ ì‹œì‘\")\n",
    "logger.write(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 2. ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì‹¤í–‰ í™˜ê²½ ì„¤ì • (Config í´ë˜ìŠ¤) ---\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    IS_SAMPLING = False     # ìƒ˜í”Œë§ ì—¬ë¶€ (True: ìƒ˜í”Œë§, False: ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
    "    SAMPLING_FRAC = 0.3     # ìƒ˜í”Œë§ ë¹„ìœ¨ (0.3 = 30%)\n",
    "    SEED = 42               # ëœë¤ ì‹œë“œ ê³ ì •\n",
    "    N_SPLITS_TS = 10        # ì‹œê³„ì—´ êµì°¨ ê²€ì¦ ë¶„í•  ìˆ˜\n",
    "    N_TOP_FEATURES = 28     # ìƒìœ„ 28ê°œ í”¼ì²˜ ì‚¬ìš©\n",
    "    N_TRIALS_OPTUNA = 30    # Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œë„ íšŸìˆ˜\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(Config.SEED)\n",
    "logger.write(\">> [1ë‹¨ê³„ ì™„ë£Œ] ë¼ì´ë¸ŒëŸ¬ë¦¬, ê²½ë¡œ, ë¡œê±° ì´ˆê¸°í™” ë° ì‹œë“œ ê³ ì • ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 3. ë°ì´í„° ë¡œë“œ ë° ë³‘í•© ---\n",
    "# ==============================================================================\n",
    "try:\n",
    "    logger.write(\"\\n>> [2ë‹¨ê³„ ì‹œì‘] ë°ì´í„° ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    train_df = pd.read_csv(TRAIN_PATH)\n",
    "    test_df = pd.read_csv(TEST_PATH)\n",
    "    submission_df = pd.read_csv(SUBMISSION_TEMPLATE_PATH)\n",
    "\n",
    "    logger.write(f\">> ì›ë³¸ ë°ì´í„° Shape - Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "    if Config.IS_SAMPLING:\n",
    "        logger.write(f\">> ìƒ˜í”Œë§ ëª¨ë“œ í™œì„±í™”: ë°ì´í„°ì˜ {Config.SAMPLING_FRAC * 100}%ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        train_df = train_df.sample(frac=Config.SAMPLING_FRAC, random_state=Config.SEED).reset_index(drop=True)\n",
    "        logger.write(f\">> ìƒ˜í”Œë§ í›„ Train Shape: {train_df.shape}\")\n",
    "\n",
    "    all_df = pd.concat([train_df.drop(columns=['target']), test_df], axis=0).reset_index(drop=True)\n",
    "    logger.write(f\">> Train/Test ë³‘í•© í›„ Shape: {all_df.shape}\")\n",
    "    logger.write(\">> [2ë‹¨ê³„ ì™„ë£Œ] ë°ì´í„° ë¡œë“œ ì„±ê³µ.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}. '{TRAIN_PATH}' ë˜ëŠ” '{TEST_PATH}' ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\", print_error=True)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 2ë‹¨ê³„(ë°ì´í„° ë¡œë“œ) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 4. ğŸ› ï¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (v6 ì•„ì´ë””ì–´ í¬í•¨ ë° ê°œì„ ) ---\n",
    "# ==============================================================================\n",
    "try:\n",
    "    logger.write(\"\\n>> [3ë‹¨ê³„ ì‹œì‘] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "\n",
    "    # --- 4.1. ë‚ ì§œ ë° ê¸°ë³¸ íŒŒìƒ ë³€ìˆ˜ ---\n",
    "    try:\n",
    "        # 'ê³„ì•½ë…„ì›”'ì„ ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (YYYYMM í˜•ì‹ì´ë¼ê³  ê°€ì •)\n",
    "        all_df['ê³„ì•½ë…„ì›”'] = pd.to_datetime(all_df['ê³„ì•½ë…„ì›”'], format='%Y%m')\n",
    "        all_df['ê³„ì•½ë…„'] = all_df['ê³„ì•½ë…„ì›”'].dt.year\n",
    "        all_df['ê³„ì•½ì›”'] = all_df['ê³„ì•½ë…„ì›”'].dt.month\n",
    "        \n",
    "        # 'ê±´ì¶•ë…„ë„'ë¥¼ 'ì—°ì‹'ìœ¼ë¡œ ìˆ˜ì •í•˜ì—¬ 'ê±´ë¬¼ë‚˜ì´' ê³„ì‚°\n",
    "        all_df['ê±´ë¬¼ë‚˜ì´'] = all_df['ê³„ì•½ë…„'] - all_df['ì—°ì‹'] \n",
    "        \n",
    "        # ì£¼ê¸°ì„± í”¼ì²˜ ìƒì„±\n",
    "        all_df['ê³„ì•½ì›”_sin'] = np.sin(2 * np.pi * all_df['ê³„ì•½ì›”'] / 12)\n",
    "        all_df['ê³„ì•½ì›”_cos'] = np.cos(2 * np.pi * all_df['ê³„ì•½ì›”'] / 12)\n",
    "        logger.write(\">> 4.1. ë‚ ì§œ/ê¸°ë³¸/ì£¼ê¸°ì„± í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.1(ë‚ ì§œ í”¼ì²˜) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # --- 4.2. êµí†µ ê°€ì¤‘í•© í”¼ì²˜ (v6 ì•„ì´ë””ì–´) ---\n",
    "    try:\n",
    "        transport_cols = ['ë°˜ê²½_1km_ì§€í•˜ì² ì—­_ìˆ˜', 'ë°˜ê²½_500m_ì§€í•˜ì² ì—­_ìˆ˜', 'ë°˜ê²½_300m_ì§€í•˜ì² ì—­_ìˆ˜',\n",
    "                          'ë°˜ê²½_1km_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜', 'ë°˜ê²½_500m_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜', 'ë°˜ê²½_300m_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜']\n",
    "        if all(col in all_df.columns for col in transport_cols):\n",
    "            all_df['ê°€ì¤‘ì§€í•˜ì² '] = all_df['ë°˜ê²½_1km_ì§€í•˜ì² ì—­_ìˆ˜'] * 1.0 + all_df['ë°˜ê²½_500m_ì§€í•˜ì² ì—­_ìˆ˜'] * 1.5 + all_df['ë°˜ê²½_300m_ì§€í•˜ì² ì—­_ìˆ˜'] * 2.0\n",
    "            all_df['ê°€ì¤‘ë²„ìŠ¤'] = all_df['ë°˜ê²½_1km_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜'] * 1.0 + all_df['ë°˜ê²½_500m_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜'] * 1.5 + all_df['ë°˜ê²½_300m_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜'] * 2.0\n",
    "            logger.write(\">> 4.2. êµí†µ ê°€ì¤‘í•© í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "        else:\n",
    "            logger.write(\">> 4.2. êµí†µ ê´€ë ¨ ì»¬ëŸ¼ì´ ì—†ì–´ ê°€ì¤‘í•© í”¼ì²˜ë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.2(êµí†µ í”¼ì²˜) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # --- 4.3. K-Means êµ°ì§‘í™” í”¼ì²˜ (v6 ì•„ì´ë””ì–´) ---\n",
    "    try:\n",
    "        cluster_features = ['ì¢Œí‘œX', 'ì¢Œí‘œY', 'ì „ìš©ë©´ì ', 'ê±´ë¬¼ë‚˜ì´', 'ì¸µ']\n",
    "        if all(col in all_df.columns for col in cluster_features):\n",
    "            scaler = StandardScaler()\n",
    "            df_scaled = scaler.fit_transform(all_df[cluster_features])\n",
    "            kmeans = KMeans(n_clusters=10, random_state=Config.SEED, n_init=10)\n",
    "            all_df['ì•„íŒŒíŠ¸êµ°ì§‘'] = kmeans.fit_predict(df_scaled)\n",
    "            logger.write(\">> 4.3. K-Means êµ°ì§‘í™” í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "        else:\n",
    "            logger.write(\">> 4.3. êµ°ì§‘í™” ê´€ë ¨ ì»¬ëŸ¼ì´ ì—†ì–´ êµ°ì§‘ í”¼ì²˜ë¥¼ ìƒì„±í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.3(K-Means) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "        \n",
    "    # --- 4.4. [ê°œì„ ] ë² ì´ìŠ¤ë¼ì¸ ì•„ì´ë””ì–´ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ---\n",
    "    # Lag/Rolling ë°©ì‹ ëŒ€ì‹ , ì‹œì¥ ë³€í™”ì— ë” ì•ˆì •ì ì¸ í†µê³„ í”¼ì²˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "    try:\n",
    "        # 1. í›ˆë ¨ ë°ì´í„°ì—ë§Œ 'ë©´ì ë‹¹ê°€ê²©' ì„ì‹œ í”¼ì²˜ ìƒì„± (Data Leakage ë°©ì§€)\n",
    "        train_only_df = all_df[all_df['target'].notna()].copy()\n",
    "        # íƒ€ê²Ÿ ë³€ìˆ˜ì— ë¡œê·¸ ë³€í™˜ í›„ ë©´ì ë‹¹ ê°€ê²© ê³„ì‚°\n",
    "        train_only_df['ë©´ì ë‹¹ê°€ê²©'] = np.log1p(train_only_df['target']) / train_only_df['ì „ìš©ë©´ì ']\n",
    "\n",
    "        # 2. ë²•ì •ë™ë³„ í†µê³„ í”¼ì²˜ ìƒì„±\n",
    "        dong_stats = train_only_df.groupby('ë²•ì •ë™').agg(\n",
    "            ë™ë³„_í‰ê· _ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'mean'),\n",
    "            ë™ë³„_std_ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'std')\n",
    "        ).reset_index()\n",
    "\n",
    "        # 3. ìì¹˜êµ¬ë³„ í†µê³„ í”¼ì²˜ ìƒì„±\n",
    "        gu_stats = train_only_df.groupby('ìì¹˜êµ¬').agg(\n",
    "            êµ¬ë³„_í‰ê· _ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'mean'),\n",
    "            êµ¬ë³„_std_ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'std')\n",
    "        ).reset_index()\n",
    "\n",
    "        # 4. ìƒì„±ëœ í†µê³„ í”¼ì²˜ë¥¼ ì „ì²´ ë°ì´í„°ì— ë³‘í•©\n",
    "        all_df = pd.merge(all_df, dong_stats, on='ë²•ì •ë™', how='left')\n",
    "        all_df = pd.merge(all_df, gu_stats, on='ìì¹˜êµ¬', how='left')\n",
    "        \n",
    "        logger.write(\">> 4.4. [ê°œì„ ] ë©´ì ë‹¹ ê°€ê²© ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.4(í†µê³„ í”¼ì²˜) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "        \n",
    "    # --- êµ°ì§‘ë³„ í†µê³„ í”¼ì²˜ ìƒì„± ---\n",
    "    try:\n",
    "        cluster_stats = train_only_df.groupby('ì•„íŒŒíŠ¸êµ°ì§‘').agg(\n",
    "            êµ°ì§‘ë³„_í‰ê· _ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'mean'),\n",
    "            êµ°ì§‘ë³„_std_ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'std')\n",
    "        ).reset_index()\n",
    "\n",
    "        all_df = pd.merge(all_df, cluster_stats, on='ì•„íŒŒíŠ¸êµ°ì§‘', how='left')\n",
    "        logger.write(\">> [ì¶”ê°€] K-Means êµ°ì§‘ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] êµ°ì§‘ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # --- 4.5. ìƒí˜¸ì‘ìš© ë° ê¸°íƒ€ í”¼ì²˜ ---\n",
    "    try:\n",
    "        all_df['ë©´ì _x_ë‚˜ì´'] = all_df['ì „ìš©ë©´ì '] * all_df['ê±´ë¬¼ë‚˜ì´']\n",
    "        all_df['ë©´ì _x_ì¸µ'] = all_df['ì „ìš©ë©´ì '] * all_df['ì¸µ']\n",
    "        all_df['ê°•ë‚¨_x_ë©´ì '] = all_df['ê°•ë‚¨3êµ¬ì—¬ë¶€'] * all_df['ì „ìš©ë©´ì ']\n",
    "        logger.write(\">> 4.5. ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.5(ìƒí˜¸ì‘ìš© í”¼ì²˜) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # --- 4.6. ìµœì¢… ì²˜ë¦¬ ---\n",
    "    try:\n",
    "        categorical_features = all_df.select_dtypes(include=['object']).columns.tolist()\n",
    "        logger.write(f\">> ì¸ì½”ë”© ëŒ€ìƒ ë²”ì£¼í˜• í”¼ì²˜: {categorical_features}\")\n",
    "        for col in categorical_features:\n",
    "            all_df[col] = LabelEncoder().fit_transform(all_df[col].astype(str))\n",
    "\n",
    "        # ë¶ˆí•„ìš”í•œ ì›ë³¸ ì»¬ëŸ¼ì´ ìˆë‹¤ë©´ ì—¬ê¸°ì„œ ì œê±°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "        all_df = all_df.drop(columns=['ê³„ì•½ë…„ì›”'])\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ ë°ì´í„° ë¶„ë¦¬\n",
    "        logger.write(f\">> ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „, NA ê°œìˆ˜: {all_df.isna().sum().sum()}\")\n",
    "        all_df = all_df.fillna(0) # ì‹œê³„ì—´ í”¼ì²˜ì—ì„œ ë°œìƒí•œ NAë¥¼ 0ìœ¼ë¡œ ì±„ì›€\n",
    "        logger.write(f\">> ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„, NA ê°œìˆ˜: {all_df.isna().sum().sum()}\")\n",
    "        \n",
    "        X_train = all_df.iloc[:len(train_df)].copy()\n",
    "        X_test = all_df.iloc[len(train_df):].copy()\n",
    "        y_train_log = np.log1p(train_df['target'])\n",
    "        \n",
    "        logger.write(f\">> ìµœì¢… í”¼ì²˜ ìˆ˜: {len(X_train.columns)}\")\n",
    "        logger.write(\">> 4.6. ë²”ì£¼í˜• ì¸ì½”ë”© ë° ìµœì¢… ë°ì´í„° ë¶„ë¦¬ ì™„ë£Œ.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.6(ìµœì¢… ì²˜ë¦¬) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    logger.write(\">> [3ë‹¨ê³„ ì™„ë£Œ] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì„±ê³µ.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 3ë‹¨ê³„(í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§) ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe16344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 5. âš¡ï¸ ë¹ ë¥¸ í”¼ì²˜ ì„ íƒ ---\n",
    "# ==============================================================================\n",
    "\n",
    "try:\n",
    "    logger.write(f\"\\n>> [4ë‹¨ê³„ ì‹œì‘] ìƒìœ„ {Config.N_TOP_FEATURES}ê°œ í”¼ì²˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\")\n",
    "    temp_model = lgb.LGBMRegressor(device='cuda', random_state=Config.SEED)\n",
    "    temp_model.fit(X_train, y_train_log)\n",
    "    feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': temp_model.feature_importances_}).sort_values('importance', ascending=False)\n",
    "    \n",
    "    logger.write(\">> í”¼ì²˜ ì¤‘ìš”ë„ :\")\n",
    "    logger.write(str(feature_importances))\n",
    "\n",
    "    all_features = X_train.columns.tolist()\n",
    "    top_features = feature_importances['feature'].head(Config.N_TOP_FEATURES).tolist()\n",
    "    discarded_features = [f for f in all_features if f not in top_features]\n",
    "    \n",
    "    X_train = X_train[top_features]\n",
    "    X_test = X_test[top_features]\n",
    "    \n",
    "    logger.write(f\">> í”¼ì²˜ ì„ íƒ ì™„ë£Œ. ì„ íƒëœ í”¼ì²˜ ìˆ˜: {len(top_features)}\")\n",
    "    logger.write(f\">> ì´ {len(all_features)}ê°œ ì¤‘ {len(top_features)}ê°œ ì„ íƒ, {len(discarded_features)}ê°œ ì œì™¸.\")\n",
    "    logger.write(f\">> ì„ íƒëœ í”¼ì²˜ ëª©ë¡ : {top_features}\")\n",
    "    logger.write(f\">> ì œì™¸ëœ í”¼ì²˜ ëª©ë¡ : {discarded_features}\")\n",
    "    logger.write(\">> [4ë‹¨ê³„ ì™„ë£Œ] í”¼ì²˜ ì„ íƒ ì„±ê³µ.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 4ë‹¨ê³„(í”¼ì²˜ ì„ íƒ) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5195fe24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 6. ğŸ§  Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ---\n",
    "# ==============================================================================\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'device': 'cuda',\n",
    "        'objective': 'regression_l1', 'metric': 'rmse', 'n_estimators': 2000,\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05),\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.7, 1.0),\n",
    "        'bagging_fraction': trial.suggest_float('bagging_fraction', 0.7, 1.0),\n",
    "        'bagging_freq': trial.suggest_int('bagging_freq', 1, 7),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 30, 100),\n",
    "        'max_depth': trial.suggest_int('max_depth', 7, 20),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 50),\n",
    "        'verbose': -1, 'n_jobs': -1, 'seed': Config.SEED,\n",
    "    }\n",
    "    \n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    model.fit(optuna_X_train, optuna_y_train, eval_set=[(optuna_X_val, optuna_y_val)], callbacks=[lgb.early_stopping(100, verbose=False)])\n",
    "    rmse = np.sqrt(mean_squared_error(optuna_y_val, model.predict(optuna_X_val)))\n",
    "    \n",
    "    # ê° trialì˜ ê²°ê³¼ë¥¼ ë¡œê¹…\n",
    "    logger.write(f\"  Trial {trial.number} | RMSE: {rmse:.5f} | Params: {trial.params}\")\n",
    "    \n",
    "    return rmse\n",
    "        \n",
    "# try:\n",
    "#     logger.write(f\"\\n>> [5ë‹¨ê³„ ì‹œì‘] Optuna ìµœì í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. (íƒìƒ‰ íšŸìˆ˜: {Config.N_TRIALS_OPTUNA})\")\n",
    "#     optuna_X_train, optuna_X_val, optuna_y_train, optuna_y_val = train_test_split(X_train, y_train_log, test_size=0.2, shuffle=False)\n",
    "#     logger.write(f\">> Optunaìš© ë°ì´í„° ë¶„í•  - Train: {optuna_X_train.shape}, Validation: {optuna_X_val.shape}\")\n",
    "\n",
    "#     study = optuna.create_study(direction='minimize')\n",
    "#     study.optimize(objective, n_trials=Config.N_TRIALS_OPTUNA)\n",
    "    \n",
    "#     best_params = study.best_params\n",
    "#     logger.write(f\">> Optuna ìµœì í™” ì™„ë£Œ. ì´ {len(study.trials)}ë²ˆì˜ trial ì‹¤í–‰.\")\n",
    "#     logger.write(f\">> ìµœì  RMSE: {study.best_value:.5f}\")\n",
    "#     logger.write(f\">> ìµœì  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_params}\")\n",
    "\n",
    "#     # ìµœì  íŒŒë¼ë¯¸í„° ì €ì¥\n",
    "#     with open(PARAMS_PATH, 'w') as f:\n",
    "#         json.dump(best_params, f, indent=4)\n",
    "#     logger.write(f\">> ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ '{PARAMS_PATH}'ì— ì €ì¥í–ˆìŠµë‹ˆë‹¤.\")\n",
    "#     logger.write(\">> [5ë‹¨ê³„ ì™„ë£Œ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì„±ê³µ.\")\n",
    "\n",
    "# except Exception as e:\n",
    "#     logger.write(f\">> [ì˜¤ë¥˜] 5ë‹¨ê³„(Optuna ìµœì í™”) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "#     logger.write(\">> [íŒíŠ¸] 'device_type = gpu' ê´€ë ¨ ì˜¤ë¥˜ëŠ” LightGBMì´ GPU ì§€ì› ì—†ì´ ì„¤ì¹˜ë˜ì—ˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\", print_error=True)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 6. ğŸš€ ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ ---\n",
    "# ==============================================================================\n",
    "try:\n",
    "    logger.write(f\"\\n>> [6ë‹¨ê³„ ì‹œì‘] ìµœì¢… ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡ì„ ì‹œì‘í•©ë‹ˆë‹¤. (CV í´ë“œ ìˆ˜: {Config.N_SPLITS_TS})\")\n",
    "    \n",
    "    # Optunaë¥¼ ìƒëµí•˜ê³ , ì´ì „ì— ì°¾ì€ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "    best_params = {\n",
    "        'learning_rate': 0.04871443094969133,\n",
    "        'feature_fraction': 0.9140552399216154,\n",
    "        'bagging_fraction': 0.8841587359791807,\n",
    "        'bagging_freq': 3,\n",
    "        'num_leaves': 100,\n",
    "        'max_depth': 20,\n",
    "        'min_child_samples': 35\n",
    "    }\n",
    "    logger.write(f\">> ë¯¸ë¦¬ ì •ì˜ëœ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {best_params}\")\n",
    "\n",
    "    # --- Learning Rate ë¯¸ì„¸ ì¡°ì • ë° ê³ ì • íŒŒë¼ë¯¸í„° ì¬ì„¤ì • ---\n",
    "    final_params = best_params.copy()\n",
    "    final_params['learning_rate'] = best_params['learning_rate'] * 0.8 \n",
    "    final_params['n_estimators'] = 3000 \n",
    "    \n",
    "    final_params.update({\n",
    "        'device': 'cuda',\n",
    "        'objective': 'regression_l1',\n",
    "        'metric': 'rmse',\n",
    "        'verbose': -1,\n",
    "        'n_jobs': -1,\n",
    "        'seed': Config.SEED\n",
    "    })\n",
    "    logger.write(f\">> ìµœì¢… í•™ìŠµ íŒŒë¼ë¯¸í„° (ë¯¸ì„¸ ì¡°ì • ë° GPU ì ìš©): {final_params}\")\n",
    "\n",
    "    # --- êµì°¨ ê²€ì¦ ë° í•™ìŠµ/ì˜ˆì¸¡ ìˆ˜í–‰ ---\n",
    "    ts_cv = TimeSeriesSplit(n_splits=Config.N_SPLITS_TS)\n",
    "    oof_preds = np.zeros(len(X_train))\n",
    "    test_preds = np.zeros(len(X_test))\n",
    "    fold_models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(ts_cv.split(X_train)):\n",
    "        logger.write(f\"--- Fold {fold+1}/{Config.N_SPLITS_TS} í•™ìŠµ ì‹œì‘ ---\")\n",
    "        \n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train_log.iloc[train_idx]\n",
    "        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train_log.iloc[val_idx]\n",
    "\n",
    "        logger.write(f\"- Train Index: {train_idx[0]} ~ {train_idx[-1]} (size: {len(train_idx)})\")\n",
    "        logger.write(f\"- Validation Index: {val_idx[0]} ~ {val_idx[-1]} (size: {len(val_idx)})\")\n",
    "        \n",
    "        model = lgb.LGBMRegressor(**final_params)\n",
    "        model.fit(X_train_fold, y_train_fold, \n",
    "                  eval_set=[(X_val_fold, y_val_fold)], \n",
    "                  eval_metric='rmse', \n",
    "                  callbacks=[lgb.early_stopping(200, verbose=False)])\n",
    "\n",
    "        val_preds = model.predict(X_val_fold)\n",
    "        oof_preds[val_idx] = val_preds\n",
    "        test_preds += model.predict(X_test) / Config.N_SPLITS_TS\n",
    "        fold_models.append(model)\n",
    "        \n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_val_fold, val_preds))\n",
    "        logger.write(f\"- Fold {fold+1} RMSE: {fold_rmse:.5f}\")\n",
    "        logger.write(f\"- Best Iteration: {model.best_iteration_}\")\n",
    "\n",
    "    # --- ìµœì¢… ê²°ê³¼ ê³„ì‚° ë° ë¡œê¹… ---\n",
    "    oof_indices = np.concatenate([val_idx for _, val_idx in ts_cv.split(X_train)])\n",
    "    oof_rmse = np.sqrt(mean_squared_error(y_train_log.iloc[oof_indices], oof_preds[oof_preds != 0]))\n",
    "    fold_scores = [np.sqrt(mean_squared_error(y_train_log.iloc[val_idx], oof_preds[val_idx])) for _, val_idx in ts_cv.split(X_train)]\n",
    "\n",
    "    logger.write(\"\\n>> CV í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "    logger.write(f\"- ê° Foldë³„ RMSE: {[round(score, 5) for score in fold_scores]}\")\n",
    "    logger.write(f\"- í‰ê·  Fold RMSE: {np.mean(fold_scores):.5f} (Â±{np.std(fold_scores):.5f})\")\n",
    "    logger.write(f\"- ì „ì²´ OOF RMSE: {oof_rmse:.5f}\")\n",
    "\n",
    "    logger.write(\">> [6ë‹¨ê³„ ì™„ë£Œ] ìµœì¢… ëª¨ë¸ í•™ìŠµ ì„±ê³µ.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 6ë‹¨ê³„(ìµœì¢… ëª¨ë¸ í•™ìŠµ) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 8. ğŸ“„ ì œì¶œ íŒŒì¼ ìƒì„± ë° ëª¨ë¸ ì €ì¥ (ìˆ˜ì • ì™„ë£Œ) ---\n",
    "# ==============================================================================\n",
    "try:\n",
    "    logger.write(\"\\n>> [7ë‹¨ê³„ ì‹œì‘] ì œì¶œ íŒŒì¼ ìƒì„± ë° ëª¨ë¸ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    final_predictions = np.expm1(test_preds)\n",
    "    final_predictions[final_predictions < 0] = 0\n",
    "    \n",
    "    submission_df = pd.DataFrame({'target': final_predictions.astype(int)})\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    logger.write(f\">> ì œì¶œ íŒŒì¼ '{SUBMISSION_PATH}' ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    logger.write(\">> ì œì¶œ íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° (ìƒìœ„ 5ê°œ):\")\n",
    "    logger.write(str(submission_df.head()))\n",
    "\n",
    "    # fold_models ë¦¬ìŠ¤íŠ¸ë¥¼ í™•ì¸í•˜ê³ , ê° ëª¨ë¸ì„ ê°œë³„ì ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "    if fold_models:\n",
    "        for i, model in enumerate(fold_models):\n",
    "            # ê° ëª¨ë¸ íŒŒì¼ëª…ì— fold ë²ˆí˜¸ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n",
    "            model_fold_path = MODEL_PATH.replace('.pkl', f'_fold{i+1}.pkl')\n",
    "            joblib.dump(model, model_fold_path)\n",
    "            logger.write(f\">> Fold {i+1} ëª¨ë¸ì´ '{model_fold_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    else:\n",
    "        logger.write(\">> [ê²½ê³ ] í•™ìŠµëœ ëª¨ë¸ì´ ì—†ì–´ ì €ì¥í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\", print_error=True)\n",
    "        \n",
    "    logger.write(\">> [7ë‹¨ê³„ ì™„ë£Œ] ì œì¶œ íŒŒì¼ ë° ëª¨ë¸ ì €ì¥ ì„±ê³µ.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 7ë‹¨ê³„(ì œì¶œ ë° ì €ì¥) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "finally:\n",
    "    logger.write(\"\\n\" + \"=\"*60)\n",
    "    logger.write(\"ğŸ‰ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    logger.write(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be24dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 9. ğŸ“Š ìµœì¢… ëª¨ë¸ ê²°ê³¼ ì‹œê°í™”, ë¶„ì„ ë° ì´ë¯¸ì§€ ì €ì¥ ---\n",
    "# ==============================================================================\n",
    "\n",
    "# IMAGE_DIR                       = '../../images/price_prediction_8/1'\n",
    "# IMAGE_FILENAME                  = 'price_prediction_8_model.pkl'\n",
    "# IMAGE_PATH                      = os.path.join(IMAGE_DIR, IMAGE_FILENAME)\n",
    "# os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    logger.write(\"\\n>> [8ë‹¨ê³„ ì‹œì‘] ëª¨ë¸ ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„ íŒŒì¼ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    logger.write(f\">> ì‹œê°í™” ê²°ê³¼ê°€ ì €ì¥ë  ê²½ë¡œ: {IMAGE_DIR}\")\n",
    "\n",
    "    # 1. í”¼ì²˜ ì¤‘ìš”ë„ ì‹œê°í™”\n",
    "    try:\n",
    "        logger.write(\">> 8.1. í”¼ì²˜ ì¤‘ìš”ë„ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        all_importances = pd.DataFrame()\n",
    "        for i, model in enumerate(fold_models):\n",
    "            fold_importance = pd.DataFrame({'feature': X_train.columns, 'importance': model.feature_importances_, 'fold': i + 1})\n",
    "            all_importances = pd.concat([all_importances, fold_importance], axis=0)\n",
    "        \n",
    "        mean_importances = all_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 10)); sns.barplot(x=mean_importances.head(20).values, y=mean_importances.head(20).index)\n",
    "        plt.title('ìƒìœ„ 20ê°œ í”¼ì²˜ ì¤‘ìš”ë„ (í‰ê· )', fontsize=16); plt.savefig(os.path.join(IMAGE_DIR, '01_feature_importance.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> í”¼ì²˜ ì¤‘ìš”ë„ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.1(í”¼ì²˜ ì¤‘ìš”ë„) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 2. ì‹¤ì œ ê°’ vs OOF ì˜ˆì¸¡ ê°’ ë¹„êµ\n",
    "    try:\n",
    "        logger.write(\">> 8.2. ì‹¤ì œ ê°’ vs OOF ì˜ˆì¸¡ ê°’ ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        plt.figure(figsize=(10, 10)); sns.scatterplot(x=np.expm1(y_train_log.iloc[oof_indices]), y=np.expm1(oof_preds[oof_preds != 0]), alpha=0.3)\n",
    "        plt.plot([0, np.expm1(y_train_log).max()], [0, np.expm1(y_train_log).max()], 'r--', lw=2)\n",
    "        plt.xlabel(\"ì‹¤ì œ ê°’ (ì›)\"); plt.ylabel(\"OOF ì˜ˆì¸¡ ê°’ (ì›)\"); plt.title('ì‹¤ì œ ê°’ vs OOF ì˜ˆì¸¡ ê°’ ë¹„êµ', fontsize=16)\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '02_actual_vs_oof_scatter.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> ì‹¤ì œ ê°’ vs OOF ì˜ˆì¸¡ ê°’ ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.2(ì‹¤ì œê°’vsì˜ˆì¸¡ê°’) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 3. ì”ì°¨ ë¶„í¬\n",
    "    try:\n",
    "        logger.write(\">> 8.3. ì”ì°¨ ë¶„í¬ í™•ì¸ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        residuals = np.expm1(y_train_log.iloc[oof_indices]) - np.expm1(oof_preds[oof_preds != 0])\n",
    "        plt.figure(figsize=(10, 6)); sns.histplot(residuals, kde=True, bins=50)\n",
    "        plt.title('ì”ì°¨(ì‹¤ì œ-ì˜ˆì¸¡) ë¶„í¬ (OOF ê¸°ë°˜)', fontsize=16); plt.xlabel(\"ì”ì°¨ (ì›)\")\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '03_residuals_distribution.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> ì”ì°¨ ë¶„í¬ í™•ì¸ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.3(ì”ì°¨ ë¶„í¬) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 4. ì˜ˆì¸¡ ë¶„í¬ ë¹„êµ\n",
    "    try:\n",
    "        logger.write(\">> 8.4. OOF ì˜ˆì¸¡ê³¼ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì˜ ë¶„í¬ ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        plt.figure(figsize=(10, 6)); sns.kdeplot(np.expm1(oof_preds[oof_preds != 0]), label='OOF ì˜ˆì¸¡ ê°’', fill=True)\n",
    "        sns.kdeplot(final_predictions, label='í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ê°’', fill=True)\n",
    "        plt.title('OOF ì˜ˆì¸¡ê³¼ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì˜ ë¶„í¬ ë¹„êµ', fontsize=16); plt.xlabel(\"ì˜ˆì¸¡ ê°’ (ì›)\"); plt.legend()\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '04_prediction_distribution_comparison.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> OOF ì˜ˆì¸¡ê³¼ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì˜ ë¶„í¬ ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.4(ì˜ˆì¸¡ ë¶„í¬) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 5. SHAP ìš”ì•½ í”Œë¡¯\n",
    "    try:\n",
    "        logger.write(\">> 8.5. SHAP ìš”ì•½ í”Œë¡¯ ë¶„ì„ ë° ì €ì¥ ì¤‘...\")\n",
    "        explainer = shap.TreeExplainer(fold_models[-1]) # ë§ˆì§€ë§‰ ëª¨ë¸ ì‚¬ìš©\n",
    "        shap_sample = X_train.sample(2000, random_state=Config.SEED) if len(X_train) > 2000 else X_train\n",
    "        shap_values = explainer.shap_values(shap_sample)\n",
    "        \n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, shap_sample, show=False)\n",
    "        plt.title(\"SHAP ìš”ì•½ í”Œë¡¯ (ë§ˆì§€ë§‰ í´ë“œ ëª¨ë¸)\", fontsize=16)\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '05_shap_summary_plot.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> SHAP ìš”ì•½ í”Œë¡¯ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.5(SHAP) ë¶„ì„ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 6. í•™ìŠµ ê³¡ì„ \n",
    "    try:\n",
    "        logger.write(\">> 8.6. í•™ìŠµ ê³¡ì„  ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        train_sizes, train_scores, validation_scores = learning_curve(\n",
    "            estimator=lgb.LGBMRegressor(**best_params, verbosity=-1, random_state=Config.SEED, device='cuda'),\n",
    "            X=X_train, y=y_train_log, train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "            cv=TimeSeriesSplit(n_splits=3), scoring='neg_root_mean_squared_error', n_jobs=-1)\n",
    "        \n",
    "        train_scores_mean = -train_scores.mean(axis=1)\n",
    "        validation_scores_mean = -validation_scores.mean(axis=1)\n",
    "\n",
    "        plt.figure(figsize=(10, 6)); plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        plt.title(\"í•™ìŠµ ê³¡ì„  (Learning Curve)\", fontsize=16); plt.xlabel(\"í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜\"); plt.ylabel(\"RMSE\"); plt.legend(loc=\"best\"); plt.grid()\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '06_learning_curve.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> í•™ìŠµ ê³¡ì„  ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.6(í•™ìŠµ ê³¡ì„ ) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    logger.write(\">> [8ë‹¨ê³„ ì™„ë£Œ] ëª¨ë“  ì‹œê°í™” ë° ë¶„ì„ íŒŒì¼ ì €ì¥ ì™„ë£Œ.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 8ë‹¨ê³„(ì‹œê°í™” ë° ë¶„ì„) ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "finally:\n",
    "    logger.write(\"\\n\" + \"=\"*60)\n",
    "    logger.write(\"ğŸ‰ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    logger.write(\"=\"*60)\n",
    "#    logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_price_predict_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

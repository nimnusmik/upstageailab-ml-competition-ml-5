{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69950ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. ë¼ì´ë¸ŒëŸ¬ë¦¬, ê²½ë¡œ, ë¡œê±° ì„¤ì • ---\n",
    "\n",
    "# ë°ì´í„° ë¶„ì„ ë° ì²˜ë¦¬ë¥¼ ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os, sys\n",
    "from datetime import datetime                                         \n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "import joblib\n",
    "import json\n",
    "\n",
    "# ì‹œê°í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "import seaborn as sns\n",
    "import shap\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# ëª¨ë¸ë§ ë° ê¸°ê³„ í•™ìŠµì„ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™”ë¥¼ ìœ„í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬\n",
    "import optuna \n",
    "\n",
    "# ë¶ˆí•„ìš”í•œ ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "\n",
    "# --- 1.1. ë¡œê±°(ì‹¤í–‰ ê¸°ë¡ ë¡œê·¸ ì €ì¥) ì„í¬íŠ¸ ---\n",
    "# ì‚¬ìš©ìê°€ ìš”ì²­í•œ ì™¸ë¶€ logger.py ëª¨ë“ˆì„ ì„í¬í•©ë‹ˆë‹¤.\n",
    "try:\n",
    "    src_path = os.path.abspath(os.path.join(os.getcwd(), \"../../src/log\"))\n",
    "    sys.path.insert(0, src_path)\n",
    "    from logger import Logger\n",
    "except ImportError:\n",
    "    print(\"ì˜¤ë¥˜: 'logger.py'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. 'src/log' ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\")\n",
    "    # ê°„ë‹¨í•œ ëŒ€ì²´ ë¡œê±° ì •ì˜\n",
    "    class Logger:\n",
    "        def __init__(self, log_path): print(f\"ëŒ€ì²´ ë¡œê±° í™œì„±í™”. ë¡œê·¸ëŠ” ê¸°ë¡ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\")\n",
    "        def write(self, message, **kwargs): print(message)\n",
    "        def start_redirect(self): pass\n",
    "        def close(self): pass\n",
    "\n",
    "# --- 1.2. í•œê¸€ í°íŠ¸ ì„¤ì • (ë‚˜ëˆ”ê³ ë”•) ---\n",
    "try:\n",
    "    font_path = '../../font/NanumFont/NanumGothic.ttf'\n",
    "    if os.path.exists(font_path):\n",
    "        fe = fm.FontEntry(fname=font_path, name='NanumGothic')\n",
    "        fm.fontManager.ttflist.insert(0, fe)\n",
    "        plt.rcParams.update({'font.size': 12, 'font.family': 'NanumGothic'})\n",
    "    else:\n",
    "        print(\"ë‚˜ëˆ”ê³ ë”• í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    pass\n",
    "\n",
    "# --- 1.3. ê²½ë¡œ ë° í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ì‚¬ìš©ì ìš”ì²­ ê¸°ë°˜) ---\n",
    "# í˜„ì¬ ì‹œê°„ ê¸°ì¤€ ë…„ì›”ì¼_ì‹œê° ë¬¸ìì—´ ìƒì„±\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "# ì‹¤í–‰ ë¡œê·¸ ì €ì¥ ê²½ë¡œ ì„¤ì •\n",
    "LOG_DIR                         = '../../data/logs/price_prediction_9_logs'\n",
    "LOG_FILENAME                    = f\"price_prediction_9_{timestamp}.log\"\n",
    "LOG_PATH                        = os.path.join(LOG_DIR, LOG_FILENAME)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "logger = Logger(log_path=LOG_PATH)\n",
    "\n",
    "# ë°ì´í„° ë° ê²°ê³¼ë¬¼ ê²½ë¡œ ì„¤ì •\n",
    "RAW_DIR                         = '../../data/processed/clean_data'\n",
    "TRAIN_FILENAME                  = 'train.csv'\n",
    "TEST_FILENAME                   = 'test.csv'\n",
    "TRAIN_PATH                      = os.path.join(RAW_DIR, TRAIN_FILENAME)\n",
    "TEST_PATH                       = os.path.join(RAW_DIR, TEST_FILENAME)\n",
    "\n",
    "PARAMS_DIR                      = '../../data/processed/params'\n",
    "PARAMS_FILENAME                 = 'best_params_9.json'\n",
    "PARAMS_PATH                     = os.path.join(PARAMS_DIR, PARAMS_FILENAME)\n",
    "\n",
    "SUBMISSION_DIR                  = '../../data/processed/submissions'\n",
    "SUBMISSION_TEMPLATE_FILENAME    = 'baseline_code_sample_submission.csv'\n",
    "SUBMISSION_FILENAME             = f'price_prediction_9_submission_{timestamp}.csv'\n",
    "SUBMISSION_TEMPLATE_PATH        = os.path.join(SUBMISSION_DIR, SUBMISSION_TEMPLATE_FILENAME)\n",
    "SUBMISSION_PATH                 = os.path.join(SUBMISSION_DIR, SUBMISSION_FILENAME)\n",
    "\n",
    "IMAGE_DIR                       = '../../images/price_prediction_9/1'\n",
    "IMAGE_FILENAME                  = 'price_prediction_9_model.pkl'\n",
    "IMAGE_PATH                      = os.path.join(IMAGE_DIR, IMAGE_FILENAME)\n",
    "\n",
    "MODEL_DIR                       = '../../model/price_prediction_9_{timestamp}'\n",
    "MODEL_FILENAME                  = 'price_prediction_9_model.pkl'\n",
    "MODEL_PATH                      = os.path.join(MODEL_DIR, MODEL_FILENAME)\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±\n",
    "os.makedirs(PARAMS_DIR, exist_ok=True)\n",
    "os.makedirs(SUBMISSION_DIR, exist_ok=True)\n",
    "os.makedirs(IMAGE_DIR, exist_ok=True)\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "logger.start_redirect()\n",
    "logger.write(\"=\"*60)\n",
    "logger.write(\">> [price_prediction8] ì•„íŒŒíŠ¸ ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ë§ ì‹œì‘\")\n",
    "logger.write(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bab7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 2. ğŸš€ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë° ì‹¤í–‰ í™˜ê²½ ì„¤ì • (Config í´ë˜ìŠ¤) ---\n",
    "# ==============================================================================\n",
    "class Config:\n",
    "    IS_SAMPLING = False     # ìƒ˜í”Œë§ ì—¬ë¶€ (True: ìƒ˜í”Œë§, False: ì „ì²´ ë°ì´í„° ì‚¬ìš©)\n",
    "    SAMPLING_FRAC = 0.3     # ìƒ˜í”Œë§ ë¹„ìœ¨ (0.3 = 30%)\n",
    "    SEED = 42               # ëœë¤ ì‹œë“œ ê³ ì •\n",
    "    N_SPLITS_TS = 10        # ì‹œê³„ì—´ êµì°¨ ê²€ì¦ ë¶„í•  ìˆ˜\n",
    "    N_TOP_FEATURES = 28     # ìƒìœ„ 28ê°œ í”¼ì²˜ ì‚¬ìš©\n",
    "    N_TRIALS_OPTUNA = 30    # Optuna í•˜ì´í¼íŒŒë¼ë¯¸í„° ìµœì í™” ì‹œë„ íšŸìˆ˜\n",
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "seed_everything(Config.SEED)\n",
    "logger.write(\">> [1ë‹¨ê³„ ì™„ë£Œ] ë¼ì´ë¸ŒëŸ¬ë¦¬, ê²½ë¡œ, ë¡œê±° ì´ˆê¸°í™” ë° ì‹œë“œ ê³ ì • ì„±ê³µ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653efc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 3. ë°ì´í„° ë¡œë“œ ë° ë³‘í•© ---\n",
    "# ==============================================================================\n",
    "try:\n",
    "    logger.write(\"\\n>> [2ë‹¨ê³„ ì‹œì‘] ë°ì´í„° ë¡œë“œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    train_df = pd.read_csv(TRAIN_PATH)\n",
    "    test_df = pd.read_csv(TEST_PATH)\n",
    "    submission_df = pd.read_csv(SUBMISSION_TEMPLATE_PATH)\n",
    "\n",
    "    logger.write(f\">> ì›ë³¸ ë°ì´í„° Shape - Train: {train_df.shape}, Test: {test_df.shape}\")\n",
    "\n",
    "    if Config.IS_SAMPLING:\n",
    "        logger.write(f\">> ìƒ˜í”Œë§ ëª¨ë“œ í™œì„±í™”: ë°ì´í„°ì˜ {Config.SAMPLING_FRAC * 100}%ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        train_df = train_df.sample(frac=Config.SAMPLING_FRAC, random_state=Config.SEED).reset_index(drop=True)\n",
    "        logger.write(f\">> ìƒ˜í”Œë§ í›„ Train Shape: {train_df.shape}\")\n",
    "        \n",
    "    logger.write(\">> [2ë‹¨ê³„ ì™„ë£Œ] ë°ì´í„° ë¡œë“œ ì„±ê³µ.\")\n",
    "\n",
    "except FileNotFoundError as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] ë°ì´í„° íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨: {e}. '{TRAIN_PATH}' ë˜ëŠ” '{TEST_PATH}' ê²½ë¡œë¥¼ í™•ì¸í•˜ì„¸ìš”.\", print_error=True)\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 2ë‹¨ê³„(ë°ì´í„° ë¡œë“œ) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f2d7a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- ì…€ 4: í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ì‚¬ìš©ì êµ¬ì¡° ë³µì› ë° ì˜¤ë¥˜ ìˆ˜ì •) ---\n",
    "# ==============================================================================\n",
    "try:\n",
    "    logger.write(\"\\n>> [3ë‹¨ê³„ ì‹œì‘] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # targetì„ í¬í•¨í•œ ìƒíƒœë¡œ concatí•˜ì—¬ í†µê³„ í”¼ì²˜ ìƒì„± ì¤€ë¹„\n",
    "    all_df = pd.concat([train_df, test_df], axis=0).reset_index(drop=True)\n",
    "    logger.write(f\">> Train/Test ë³‘í•© í›„ Shape: {all_df.shape}\")\n",
    "\n",
    "    # --- 4.1. ë‚ ì§œ ë° ê¸°ë³¸ íŒŒìƒ ë³€ìˆ˜ ---\n",
    "    try:\n",
    "        # 'ê³„ì•½ë…„ì›”'ì„ ë‚ ì§œ íƒ€ì…ìœ¼ë¡œ ë³€í™˜ (YYYYMM í˜•ì‹ì´ë¼ê³  ê°€ì •)\n",
    "        all_df['ê³„ì•½ë…„ì›”'] = pd.to_datetime(all_df['ê³„ì•½ë…„ì›”'], format='%Y%m')\n",
    "        # 'ê³„ì•½ë…„'ê³¼ 'ê³„ì•½ì›”' í”¼ì²˜ ìƒì„±\n",
    "        all_df['ê³„ì•½ë…„'] = all_df['ê³„ì•½ë…„ì›”'].dt.year\n",
    "        all_df['ê³„ì•½ì›”'] = all_df['ê³„ì•½ë…„ì›”'].dt.month\n",
    "        # 'ê±´ë¬¼ë‚˜ì´' í”¼ì²˜ ìƒì„±\n",
    "        all_df['ê±´ë¬¼ë‚˜ì´'] = all_df['ê³„ì•½ë…„'] - all_df['ì—°ì‹'] \n",
    "        # ì›”(Month) í”¼ì²˜ë¥¼ ì£¼ê¸°ì„±ì„ ê°€ì§€ë„ë¡ sin, cos ë³€í™˜\n",
    "        all_df['ê³„ì•½ì›”_sin'] = np.sin(2 * np.pi * all_df['ê³„ì•½ì›”'] / 12)\n",
    "        all_df['ê³„ì•½ì›”_cos'] = np.cos(2 * np.pi * all_df['ê³„ì•½ì›”'] / 12)\n",
    "        logger.write(\">> 4.1. ë‚ ì§œ/ê¸°ë³¸/ì£¼ê¸°ì„± í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.1(ë‚ ì§œ í”¼ì²˜) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # --- 4.2. êµí†µ ê°€ì¤‘í•© í”¼ì²˜ ---\n",
    "    try:\n",
    "        # ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•œ êµí†µ í¸ì˜ì„± í”¼ì²˜\n",
    "        all_df['ê°€ì¤‘ì§€í•˜ì² '] = all_df['ë°˜ê²½_1km_ì§€í•˜ì² ì—­_ìˆ˜']*1.0 + all_df['ë°˜ê²½_500m_ì§€í•˜ì² ì—­_ìˆ˜']*1.5 + all_df['ë°˜ê²½_300m_ì§€í•˜ì² ì—­_ìˆ˜']*2.0\n",
    "        all_df['ê°€ì¤‘ë²„ìŠ¤'] = all_df['ë°˜ê²½_1km_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜']*1.0 + all_df['ë°˜ê²½_500m_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜']*1.5 + all_df['ë°˜ê²½_300m_ë²„ìŠ¤ì •ë¥˜ì¥_ìˆ˜']*2.0\n",
    "        logger.write(\">> 4.2. êµí†µ ê°€ì¤‘í•© í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.2(êµí†µ í”¼ì²˜) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # --- 4.3. K-Means êµ°ì§‘í™” í”¼ì²˜ ---\n",
    "    try:\n",
    "        # ì£¼ìš” ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì•„íŒŒíŠ¸ êµ°ì§‘ ìƒì„±\n",
    "        cluster_features = ['ì¢Œí‘œX', 'ì¢Œí‘œY', 'ì „ìš©ë©´ì ', 'ê±´ë¬¼ë‚˜ì´', 'ì¸µ']\n",
    "        scaler = StandardScaler()\n",
    "        df_scaled = scaler.fit_transform(all_df[cluster_features])\n",
    "        kmeans = KMeans(n_clusters=10, random_state=Config.SEED, n_init=10)\n",
    "        all_df['ì•„íŒŒíŠ¸êµ°ì§‘'] = kmeans.fit_predict(df_scaled)\n",
    "        logger.write(\">> 4.3. K-Means êµ°ì§‘í™” í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.3(K-Means) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "        \n",
    "    # --- 4.4. í†µê³„ í”¼ì²˜ ìƒì„± ---\n",
    "    try:\n",
    "        # ë°ì´í„° ìœ ì¶œì„ ë°©ì§€í•˜ë©°, ì§€ì—­/êµ°ì§‘ë³„ ê°€ê²© í†µê³„ í”¼ì²˜ ìƒì„±\n",
    "        train_only_df_mask = all_df['target'].notna()\n",
    "        all_df.loc[train_only_df_mask, 'ë©´ì ë‹¹ê°€ê²©'] = np.log1p(all_df.loc[train_only_df_mask, 'target']) / all_df.loc[train_only_df_mask, 'ì „ìš©ë©´ì ']\n",
    "        \n",
    "        dong_stats = all_df[train_only_df_mask].groupby('ë²•ì •ë™').agg(ë™ë³„_í‰ê· _ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'mean'), ë™ë³„_std_ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'std')).reset_index()\n",
    "        gu_stats = all_df[train_only_df_mask].groupby('ìì¹˜êµ¬').agg(êµ¬ë³„_í‰ê· _ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'mean'), êµ¬ë³„_std_ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'std')).reset_index()\n",
    "        cluster_stats = all_df[train_only_df_mask].groupby('ì•„íŒŒíŠ¸êµ°ì§‘').agg(êµ°ì§‘ë³„_í‰ê· _ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'mean'), êµ°ì§‘ë³„_std_ë©´ì ë‹¹ê°€ê²©=('ë©´ì ë‹¹ê°€ê²©', 'std')).reset_index()\n",
    "        \n",
    "        all_df = pd.merge(all_df, dong_stats, on='ë²•ì •ë™', how='left')\n",
    "        all_df = pd.merge(all_df, gu_stats, on='ìì¹˜êµ¬', how='left')\n",
    "        all_df = pd.merge(all_df, cluster_stats, on='ì•„íŒŒíŠ¸êµ°ì§‘', how='left')\n",
    "        all_df = all_df.drop(columns=['ë©´ì ë‹¹ê°€ê²©'])\n",
    "        logger.write(\">> 4.4. ë²•ì •ë™/ìì¹˜êµ¬/ì•„íŒŒíŠ¸êµ°ì§‘ ê¸°ë°˜ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.4(í†µê³„ í”¼ì²˜) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "        \n",
    "    # --- 4.5. ìƒí˜¸ì‘ìš© í”¼ì²˜ ---\n",
    "    try:\n",
    "        # ë³€ìˆ˜ ê°„ì˜ ìƒí˜¸ì‘ìš©ì„ ê³ ë ¤í•œ íŒŒìƒë³€ìˆ˜ ìƒì„±\n",
    "        all_df['ë©´ì _x_ë‚˜ì´'] = all_df['ì „ìš©ë©´ì '] * all_df['ê±´ë¬¼ë‚˜ì´']\n",
    "        all_df['ë©´ì _x_ì¸µ'] = all_df['ì „ìš©ë©´ì '] * all_df['ì¸µ']\n",
    "        all_df['ê°•ë‚¨_x_ë©´ì '] = all_df['ê°•ë‚¨3êµ¬ì—¬ë¶€'] * all_df['ì „ìš©ë©´ì ']\n",
    "        logger.write(\">> 4.5. ìƒí˜¸ì‘ìš© í”¼ì²˜ ìƒì„± ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.5(ìƒí˜¸ì‘ìš© í”¼ì²˜) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # --- 4.6. ìµœì¢… ì²˜ë¦¬ ---\n",
    "    try:\n",
    "        # ëª¨ë¸ í•™ìŠµì— ì‚¬ìš©í•˜ì§€ ì•Šì„ ì›ë³¸ ì»¬ëŸ¼ë“¤ì„ ëª…ì‹œì ìœ¼ë¡œ ì œê±°\n",
    "        cols_to_drop = ['ê³„ì•½ë…„ì›”', 'ê³„ì•½ì¼ì']\n",
    "        all_df = all_df.drop(columns=cols_to_drop)\n",
    "        \n",
    "        # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„ ë°ì´í„° ë¶„ë¦¬\n",
    "        logger.write(f\">> ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì „, NA ê°œìˆ˜: {all_df.isna().sum().sum()}\")\n",
    "        all_df = all_df.fillna(0) # ì‹œê³„ì—´ í”¼ì²˜ì—ì„œ ë°œìƒí•œ NAë¥¼ 0ìœ¼ë¡œ ì±„ì›€\n",
    "        logger.write(f\">> ê²°ì¸¡ì¹˜ ì²˜ë¦¬ í›„, NA ê°œìˆ˜: {all_df.isna().sum().sum()}\")\n",
    "        \n",
    "        # ë°ì´í„°ë¥¼ í›ˆë ¨/í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¶„ë¦¬í•˜ê³ , íƒ€ê²Ÿ ë³€ìˆ˜(y) ìƒì„±\n",
    "        train_processed = all_df[all_df['target'] != 0].copy()\n",
    "        X_train = train_processed.drop(columns=['target'])\n",
    "        y_train_log = np.log1p(train_processed['target'])\n",
    "        X_test = all_df[all_df['target'] == 0].drop(columns=['target']).copy()\n",
    "        \n",
    "        logger.write(f\">> ìµœì¢… í”¼ì²˜ ìˆ˜: {len(X_train.columns)}\")\n",
    "        logger.write(\">> 4.6. ìµœì¢… ë°ì´í„° ë¶„ë¦¬ ë° ì „ì²˜ë¦¬ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 4.6(ìµœì¢… ì²˜ë¦¬) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    logger.write(\">> [3ë‹¨ê³„ ì™„ë£Œ] í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì„±ê³µ.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 3ë‹¨ê³„(í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§) ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe16344",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 5. âš¡ï¸ ë¹ ë¥¸ í”¼ì²˜ ì„ íƒ ---\n",
    "# ==============================================================================\n",
    "\n",
    "try:\n",
    "    logger.write(f\"\\n>> [4ë‹¨ê³„ ì‹œì‘] ìƒìœ„ {Config.N_TOP_FEATURES}ê°œ í”¼ì²˜ë¥¼ ì„ íƒí•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # í”¼ì²˜ ì„ íƒì„ ìœ„í•œ ì„ì‹œ ë°ì´í„°í”„ë ˆì„ ë³µì‚¬\n",
    "    X_train_fs = X_train.copy()\n",
    "    \n",
    "    # ë²”ì£¼í˜• í”¼ì²˜ë“¤ì˜ íƒ€ì…ì„ LightGBMì´ ì¸ì‹í•  ìˆ˜ ìˆëŠ” 'category'ë¡œ ë³€í™˜\n",
    "    categorical_fs = ['ìì¹˜êµ¬', 'ë²•ì •ë™', 'ë¸Œëœë“œë“±ê¸‰', 'ì•„íŒŒíŠ¸êµ°ì§‘']\n",
    "    for col in categorical_fs:\n",
    "        if col in X_train_fs.columns:\n",
    "            X_train_fs[col] = X_train_fs[col].astype('category')\n",
    "\n",
    "    # ì„ì‹œ ëª¨ë¸ë¡œ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°\n",
    "    temp_model = lgb.LGBMRegressor(device='cuda', random_state=Config.SEED)\n",
    "    temp_model.fit(X_train_fs, y_train_log)\n",
    "    \n",
    "    feature_importances = pd.DataFrame({'feature': X_train.columns, 'importance': temp_model.feature_importances_})\n",
    "    feature_importances = feature_importances.sort_values('importance', ascending=False)\n",
    "    logger.write(\">> í”¼ì²˜ ì¤‘ìš”ë„ :\")\n",
    "    logger.write(str(feature_importances))\n",
    "    \n",
    "    # ì „ì²´ í”¼ì²˜ ëª©ë¡\n",
    "    all_features = X_train.columns.tolist()\n",
    "    \n",
    "    # ì¤‘ìš”ë„ ìƒìœ„ í”¼ì²˜ë§Œ ì„ íƒ\n",
    "    top_features = feature_importances['feature'].head(Config.N_TOP_FEATURES).tolist()\n",
    "    X_train = X_train[top_features]\n",
    "    X_test = X_test[top_features]\n",
    "    logger.write(f\">> ì„ íƒëœ ìƒìœ„ í”¼ì³ ìˆ˜: {len(top_features)}\")\n",
    "    logger.write(f\">> ì„ íƒëœ ìƒìœ„ í”¼ì³: {top_features}\")\n",
    "    \n",
    "    # í”¼ì²˜ ì„ íƒ í›„, ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í”¼ì²˜ ëª©ë¡\n",
    "    discarded_features = [f for f in all_features if f not in top_features]\n",
    "    logger.write(f\">> ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í”¼ì²˜ ìˆ˜: {len(discarded_features)}\")\n",
    "    logger.write(f\">> ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” í”¼ì²˜: {discarded_features}\")\n",
    "\n",
    "    logger.write(f\">> ìµœì¢… í”¼ì³ ê°œìˆ˜: ì´ {len(all_features)}ê°œ ì¤‘ {len(top_features)}ê°œ ì„ íƒ, {len(discarded_features)}ê°œ ì œì™¸.\")\n",
    "    logger.write(\">> [4ë‹¨ê³„ ì™„ë£Œ] í”¼ì²˜ ì„ íƒ ì„±ê³µ.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 4ë‹¨ê³„(í”¼ì²˜ ì„ íƒ) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301f310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 6. LightGBM ëª¨ë¸ í•™ìŠµ ---\n",
    "# ==============================================================================\n",
    "try:\n",
    "    logger.write(f\"\\n>> [5ë‹¨ê³„ ì‹œì‘] LightGBM ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. (CV í´ë“œ ìˆ˜: {Config.N_SPLITS_TS})\")\n",
    "    \n",
    "    # ì‚¬ìš©ì ì •ì˜ ìµœì ì˜ íŒŒë¼ë¯¸í„°\n",
    "    best_params = { \n",
    "        'learning_rate': 0.04, \n",
    "        'feature_fraction': 0.9, \n",
    "        'bagging_fraction': 0.85, \n",
    "        'bagging_freq': 3, \n",
    "        'num_leaves': 100, \n",
    "        'max_depth': 20, \n",
    "        'min_child_samples': 35 }\n",
    "    logger.write(f\">> ì‚¬ìš©ìê°€ ì •ì˜í•œ ìµœì  íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤: {best_params}\")\n",
    "    \n",
    "    # ìµœì¢… í•™ìŠµì„ ìœ„í•œ íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •\n",
    "    final_params = best_params.copy()\n",
    "    final_params['learning_rate'] *= 0.8\n",
    "    final_params['n_estimators'] = 3000\n",
    "    final_params.update({\n",
    "        'device': 'cuda', \n",
    "        'objective': 'regression_l1', \n",
    "        'metric': 'rmse', \n",
    "        'verbose': -1, \n",
    "        'n_jobs': -1, \n",
    "        'seed': Config.SEED})\n",
    "    logger.write(f\">> ìµœì¢… LGBM í•™ìŠµ íŒŒë¼ë¯¸í„°: {final_params}\")\n",
    "    \n",
    "    # LGBM í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„\n",
    "    X_train_lgbm, X_test_lgbm = X_train.copy(), X_test.copy()\n",
    "    categorical_cols_lgbm = ['ìì¹˜êµ¬', 'ë²•ì •ë™', 'ë¸Œëœë“œë“±ê¸‰', 'ì•„íŒŒíŠ¸êµ°ì§‘']\n",
    "    \n",
    "    logger.write(\">> LightGBM í•™ìŠµì„ ìœ„í•´ ë²”ì£¼í˜• í”¼ì²˜ íƒ€ì…ì„ 'category'ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\")\n",
    "    logger.write(f\">> ëŒ€ìƒ ì»¬ëŸ¼: {categorical_cols_lgbm}\")\n",
    "    \n",
    "    for col in categorical_cols_lgbm:\n",
    "        if col in X_train_lgbm.columns:\n",
    "            X_train_lgbm[col] = X_train_lgbm[col].astype('category')\n",
    "            X_test_lgbm[col] = X_test_lgbm[col].astype('category')\n",
    "    logger.write(\">> 'category' íƒ€ì… ë³€í™˜ ì™„ë£Œ.\")\n",
    "            \n",
    "    # TimeSeriesSplit êµì°¨ ê²€ì¦\n",
    "    ts_cv = TimeSeriesSplit(n_splits=Config.N_SPLITS_TS)\n",
    "    lgbm_fold_models = []\n",
    "    lgbm_oof_preds = np.zeros(len(X_train_lgbm))\n",
    "    lgbm_test_preds = np.zeros(len(X_test_lgbm))\n",
    "    lgbm_fold_scores = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(ts_cv.split(X_train_lgbm)):\n",
    "        logger.write(f\"--- LightGBM Fold {fold+1}/{Config.N_SPLITS_TS} í•™ìŠµ ì‹œì‘ ---\")\n",
    "        X_train_fold, y_train_fold = X_train_lgbm.iloc[train_idx], y_train_log.iloc[train_idx]\n",
    "        X_val_fold, y_val_fold = X_train_lgbm.iloc[val_idx], y_train_log.iloc[val_idx]\n",
    "        \n",
    "        # ë°ì´í„° ë¶„í•  ì •ë³´ ë¡œê¹…\n",
    "        logger.write(f\"- Train Index: {train_idx[0]} ~ {train_idx[-1]} (size: {len(train_idx)})\")\n",
    "        logger.write(f\"- Validation Index: {val_idx[0]} ~ {val_idx[-1]} (size: {len(val_idx)})\")\n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        model = lgb.LGBMRegressor(**final_params)\n",
    "        model.fit(X_train_fold, y_train_fold, \n",
    "                  eval_set=[(X_val_fold, y_val_fold)], \n",
    "                  eval_metric='rmse', \n",
    "                  callbacks=[lgb.early_stopping(200, verbose=False)])\n",
    "        \n",
    "        # OOF ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "        val_preds = model.predict(X_val_fold)\n",
    "        lgbm_oof_preds[val_idx] = model.predict(X_val_fold)\n",
    "        lgbm_test_preds += model.predict(X_test_lgbm) / Config.N_SPLITS_TS\n",
    "        lgbm_fold_models.append(model)\n",
    "        \n",
    "        # í•´ë‹¹ Foldì˜ ê²°ê³¼ ë¡œê¹…\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_val_fold, val_preds))\n",
    "        lgbm_fold_scores.append(fold_rmse)\n",
    "        logger.write(f\"- Fold {fold+1} RMSE: {fold_rmse:.5f}\")\n",
    "        logger.write(f\"- Best Iteration: {model.best_iteration_}\")\n",
    "    \n",
    "    # CV í•™ìŠµ ê²°ê³¼ ìš”ì•½ ë¡œê·¸\n",
    "    oof_rmse = np.sqrt(mean_squared_error(y_train_log, lgbm_oof_preds))\n",
    "    logger.write(\"\\n>> LightGBM CV í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "    logger.write(f\"- ê° Foldë³„ RMSE: {[round(score, 5) for score in lgbm_fold_scores]}\")\n",
    "    logger.write(f\"- í‰ê·  Fold RMSE: {np.mean(lgbm_fold_scores):.5f} (Â±{np.std(lgbm_fold_scores):.5f})\")\n",
    "    logger.write(f\"- ì „ì²´ OOF RMSE: {oof_rmse:.5f}\")\n",
    "    logger.write(\">> [5ë‹¨ê³„ ì™„ë£Œ] LightGBM ëª¨ë¸ í•™ìŠµ ì„±ê³µ.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 5ë‹¨ê³„(LightGBM í•™ìŠµ) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3b1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 7. CatBoost ëª¨ë¸ í•™ìŠµ ---\n",
    "# ==============================================================================\n",
    "\n",
    "try:\n",
    "    logger.write(f\"\\n>> [6ë‹¨ê³„ ì‹œì‘] CatBoost ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. (CV í´ë“œ ìˆ˜: {Config.N_SPLITS_TS})\")\n",
    "    \n",
    "    # CatBoostê°€ ì²˜ë¦¬í•  ë²”ì£¼í˜• í”¼ì²˜ ëª©ë¡ ì •ì˜\n",
    "    categorical_cols_cat = ['ìì¹˜êµ¬', 'ë²•ì •ë™', 'ë¸Œëœë“œë“±ê¸‰', 'ì•„íŒŒíŠ¸êµ°ì§‘']\n",
    "    existing_categorical_cols = [col for col in categorical_cols_cat if col in X_train.columns]\n",
    "    logger.write(f\">> CatBoost ë²”ì£¼í˜• í”¼ì²˜: {existing_categorical_cols}\")\n",
    "\n",
    "    ts_cv = TimeSeriesSplit(n_splits=Config.N_SPLITS_TS)\n",
    "    cat_fold_models, cat_oof_preds = [], np.zeros(len(X_train))\n",
    "    cat_test_preds = np.zeros(len(X_test))\n",
    "    cat_fold_scores = []\n",
    "    \n",
    "    # CatBoost í•™ìŠµ íŒŒë¼ë¯¸í„°\n",
    "    cat_params = {\n",
    "        'iterations': 3000, \n",
    "        'learning_rate': 0.05, \n",
    "        'depth': 8, \n",
    "        'l2_leaf_reg': 3, \n",
    "        'loss_function': 'RMSE', \n",
    "        'eval_metric': 'RMSE', \n",
    "        'random_seed': Config.SEED, \n",
    "        'verbose': 0, \n",
    "        'task_type': 'GPU'}\n",
    "    logger.write(f\">> CatBoost í•™ìŠµ íŒŒë¼ë¯¸í„°: {cat_params}\")\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(ts_cv.split(X_train)):\n",
    "        logger.write(f\"--- CatBoost Fold {fold+1}/{Config.N_SPLITS_TS} í•™ìŠµ ì‹œì‘ ---\")\n",
    "        \n",
    "        X_train_fold, y_train_fold = X_train.iloc[train_idx], y_train_log.iloc[train_idx]\n",
    "        X_val_fold, y_val_fold = X_train.iloc[val_idx], y_train_log.iloc[val_idx]\n",
    "        \n",
    "        # ë°ì´í„° ë¶„í•  ì •ë³´ ë¡œê¹…\n",
    "        logger.write(f\"- Train Index: {train_idx[0]} ~ {train_idx[-1]} (size: {len(train_idx)})\")\n",
    "        logger.write(f\"- Validation Index: {val_idx[0]} ~ {val_idx[-1]} (size: {len(val_idx)})\")\n",
    "        \n",
    "        # ëª¨ë¸ í•™ìŠµ\n",
    "        cat_model = CatBoostRegressor(**cat_params)\n",
    "        cat_model.fit(X_train_fold, y_train_fold, \n",
    "                      eval_set=[(X_val_fold, y_val_fold)], \n",
    "                      early_stopping_rounds=200, \n",
    "                      cat_features=existing_categorical_cols, \n",
    "                      verbose=0)\n",
    "        \n",
    "        # OOF ë° í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡\n",
    "        val_preds = cat_model.predict(X_val_fold)\n",
    "        cat_oof_preds[val_idx] = val_preds\n",
    "        cat_test_preds += cat_model.predict(X_test) / Config.N_SPLITS_TS\n",
    "        cat_fold_models.append(cat_model)\n",
    "        \n",
    "        # í•´ë‹¹ Foldì˜ ê²°ê³¼ ë¡œê¹…\n",
    "        fold_rmse = np.sqrt(mean_squared_error(y_val_fold, val_preds))\n",
    "        cat_fold_scores.append(fold_rmse)\n",
    "        logger.write(f\"- Fold {fold+1} RMSE: {fold_rmse:.5f}\")\n",
    "        logger.write(f\"- Best Iteration: {cat_model.get_best_iteration()}\")\n",
    "    \n",
    "    # CV í•™ìŠµ ê²°ê³¼ ìš”ì•½ ë¡œê·¸\n",
    "    oof_rmse = np.sqrt(mean_squared_error(y_train_log, cat_oof_preds))\n",
    "    logger.write(\"\\n>> CatBoost CV í•™ìŠµ ê²°ê³¼ ìš”ì•½:\")\n",
    "    logger.write(f\"- ê° Foldë³„ RMSE: {[round(score, 5) for score in cat_fold_scores]}\")\n",
    "    logger.write(f\"- í‰ê·  Fold RMSE: {np.mean(cat_fold_scores):.5f} (Â±{np.std(cat_fold_scores):.5f})\")\n",
    "    logger.write(f\"- ì „ì²´ OOF RMSE: {oof_rmse:.5f}\")\n",
    "    logger.write(\">> [6ë‹¨ê³„ ì™„ë£Œ] CatBoost ëª¨ë¸ í•™ìŠµ ì„±ê³µ.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 6ë‹¨ê³„(CatBoost í•™ìŠµ) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba8ed28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 8. ğŸš€ ìµœì¢… ì˜ˆì¸¡ ê²°í•© ë° ì œì¶œ ---\n",
    "# ==============================================================================\n",
    "\n",
    "try:\n",
    "    logger.write(\"\\n>> [7ë‹¨ê³„ ì‹œì‘] ìµœì¢… ì˜ˆì¸¡ ê²°í•© ë° ì œì¶œì„ ì‹œì‘í•©ë‹ˆë‹¤.\")\n",
    "    \n",
    "    # ê° ëª¨ë¸ì˜ OOF(Out-of-Fold) ê²€ì¦ ì ìˆ˜ ê³„ì‚°\n",
    "    lgbm_oof_rmse = np.sqrt(mean_squared_error(y_train_log, lgbm_oof_preds))\n",
    "    cat_oof_rmse = np.sqrt(mean_squared_error(y_train_log, cat_oof_preds))\n",
    "    logger.write(f\">> LightGBM OOF RMSE: {lgbm_oof_rmse:.5f}\")\n",
    "    logger.write(f\">> CatBoost OOF RMSE: {cat_oof_rmse:.5f}\")\n",
    "\n",
    "    # ì•™ìƒë¸” ê°€ì¤‘ì¹˜ ì„¤ì • ë° ì˜ˆì¸¡ ê²°í•©\n",
    "    lgbm_weight, cat_weight = 0.6, 0.4\n",
    "    ensembled_preds = lgbm_test_preds * lgbm_weight + cat_test_preds * cat_weight\n",
    "    logger.write(f\">> ì ìš©ëœ ì•™ìƒë¸” ê°€ì¤‘ì¹˜ - LGBM: {lgbm_weight}, CatBoost: {cat_weight}\")\n",
    "    \n",
    "    # ì•™ìƒë¸”ëœ ì˜ˆì¸¡ê°’(ë¡œê·¸ ìŠ¤ì¼€ì¼)ì˜ í†µê³„ ì •ë³´ ë¡œê¹…\n",
    "    logger.write(f\">> ì•™ìƒë¸” ì˜ˆì¸¡ê°’(ë¡œê·¸ ìŠ¤ì¼€ì¼) ìš”ì•½: Min({np.min(ensembled_preds):.4f}), Max({np.max(ensembled_preds):.4f}), Mean({np.mean(ensembled_preds):.4f})\")\n",
    "    \n",
    "    # ìµœì¢… ì˜ˆì¸¡ê°’ ìŠ¤ì¼€ì¼ ë³µì›\n",
    "    final_predictions = np.expm1(ensembled_preds)\n",
    "    \n",
    "    # ìŒìˆ˜ ê°’ ì²˜ë¦¬ ì „, í›„ ì •ë³´ ë¡œê¹…\n",
    "    negative_preds_count = (final_predictions < 0).sum()\n",
    "    logger.write(f\">> ìŒìˆ˜ë¡œ ì˜ˆì¸¡ëœ ê°’ì˜ ìˆ˜: {negative_preds_count}ê°œ\")\n",
    "    final_predictions[final_predictions < 0] = 0\n",
    "    \n",
    "    # ìµœì¢… ì˜ˆì¸¡ê°’(ì›ë˜ ìŠ¤ì¼€ì¼)ì˜ í†µê³„ ì •ë³´ ë¡œê¹…\n",
    "    logger.write(f\">> ìµœì¢… ì˜ˆì¸¡ê°’(ì›ë˜ ìŠ¤ì¼€ì¼) ìš”ì•½: Min({np.min(final_predictions):.2f}), Max({np.max(final_predictions):.2f}), Mean({np.mean(final_predictions):.2f})\")\n",
    "\n",
    "    # ì œì¶œ íŒŒì¼ ìƒì„±\n",
    "    submission_df['target'] = final_predictions.astype(int)\n",
    "    submission_df.to_csv(SUBMISSION_PATH, index=False)\n",
    "    logger.write(f\">> ì œì¶œ íŒŒì¼ '{SUBMISSION_PATH}' ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    # ì œì¶œ íŒŒì¼ í˜•íƒœ ë¡œê¹…\n",
    "    logger.write(f\">> ìµœì¢… ì œì¶œ íŒŒì¼ Shape: {submission_df.shape}\")\n",
    "    \n",
    "    # í•™ìŠµëœ ëª¨ë“  ëª¨ë¸ ì €ì¥\n",
    "    for i, model in enumerate(lgbm_fold_models):\n",
    "        joblib.dump(model, os.path.join(MODEL_DIR, f'lgbm_fold{i+1}.pkl'))\n",
    "    for i, model in enumerate(cat_fold_models):\n",
    "        joblib.dump(model, os.path.join(MODEL_DIR, f'cat_fold{i+1}.pkl'))\n",
    "    logger.write(\">> ëª¨ë“  Fold ëª¨ë¸ ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    \n",
    "    logger.write(\">> [7ë‹¨ê³„ ì™„ë£Œ] ì œì¶œ íŒŒì¼ ë° ëª¨ë¸ ì €ì¥ ì„±ê³µ.\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 7ë‹¨ê³„(ì œì¶œ ë° ì €ì¥) ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be24dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# --- 9. ğŸ“Š ìµœì¢… ëª¨ë¸ ê²°ê³¼ ì‹œê°í™”, ë¶„ì„ ë° ì´ë¯¸ì§€ ì €ì¥ (ìµœì¢… ìˆ˜ì •) ---\n",
    "# ==============================================================================\n",
    "\n",
    "# --- 1.2. í•œê¸€ í°íŠ¸ ì„¤ì • (ë‚˜ëˆ”ê³ ë”•) ---\n",
    "import matplotlib.font_manager as fm\n",
    "try:\n",
    "    font_path = '../../font/NanumFont/NanumGothic.ttf'\n",
    "    if os.path.exists(font_path):\n",
    "        fe = fm.FontEntry(fname=font_path, name='NanumGothic')\n",
    "        fm.fontManager.ttflist.insert(0, fe)\n",
    "        plt.rcParams.update({'font.size': 12, 'font.family': 'NanumGothic'})\n",
    "    else:\n",
    "        print(\"ë‚˜ëˆ”ê³ ë”• í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ ê¸°ë³¸ í°íŠ¸ë¡œ ì„¤ì •ë©ë‹ˆë‹¤.\")\n",
    "except Exception as e:\n",
    "    print(f\"í°íŠ¸ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    logger.write(\"\\n>> [8ë‹¨ê³„ ì‹œì‘] ëª¨ë¸ ê²°ê³¼ ì‹œê°í™” ë° ë¶„ì„ íŒŒì¼ ì €ì¥ì„ ì‹œì‘í•©ë‹ˆë‹¤...\")\n",
    "    logger.write(f\">> ì‹œê°í™” ê²°ê³¼ê°€ ì €ì¥ë  ê²½ë¡œ: {IMAGE_DIR}\")\n",
    "\n",
    "    # [ì •ë¦¬] ì‹œê°í™”ì— í•„ìš”í•œ ë³€ìˆ˜ë“¤ì„ ì´ ë‹¨ê³„ ì‹œì‘ ë¶€ë¶„ì—ì„œ ëª…í™•í•˜ê²Œ ì •ì˜í•©ë‹ˆë‹¤.\n",
    "    \n",
    "    # 1. ì•™ìƒë¸” OOF(Out-of-Fold) ì˜ˆì¸¡ê°’ ë° ê´€ë ¨ ë³€ìˆ˜ ìƒì„±\n",
    "    ensembled_oof_preds = lgbm_oof_preds * lgbm_weight + cat_oof_preds * cat_weight\n",
    "    residuals = np.expm1(y_train_log) - np.expm1(ensembled_oof_preds)\n",
    "    \n",
    "    # 2. LightGBM ëª¨ë¸ ê¸°ë°˜ í”¼ì²˜ ì¤‘ìš”ë„ ê³„ì‚°\n",
    "    all_importances = pd.DataFrame()\n",
    "    for i, model in enumerate(lgbm_fold_models):\n",
    "        fold_importance = pd.DataFrame({'feature': X_train.columns, 'importance': model.feature_importances_, 'fold': i + 1})\n",
    "        all_importances = pd.concat([all_importances, fold_importance], axis=0)\n",
    "    mean_importances = all_importances.groupby('feature')['importance'].mean().sort_values(ascending=False)\n",
    "\n",
    "    # 3. SHAP ë¶„ì„ì„ ìœ„í•œ ë³€ìˆ˜ ì¤€ë¹„ (LGBM ë§ˆì§€ë§‰ ëª¨ë¸ ê¸°ì¤€)\n",
    "    explainer = shap.TreeExplainer(lgbm_fold_models[-1])\n",
    "    shap_sample = X_train_lgbm.sample(2000, random_state=Config.SEED) if len(X_train_lgbm) > 2000 else X_train_lgbm\n",
    "    shap_values = explainer.shap_values(shap_sample)\n",
    "\n",
    "    # --- ì´ì œë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹œê°í™”ë¥¼ ì§„í–‰í•©ë‹ˆë‹¤. ---\n",
    "\n",
    "    # 1. ì‹¤ì œ ê°’ vs OOF ì˜ˆì¸¡ ê°’ ë¹„êµ (ì•™ìƒë¸” ê¸°ì¤€)\n",
    "    try:\n",
    "        logger.write(\">> 8.1. ì‹¤ì œ ê°’ vs OOF ì˜ˆì¸¡ ê°’ ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        plt.figure(figsize=(10, 10)); sns.scatterplot(x=np.expm1(y_train_log), y=np.expm1(ensembled_oof_preds), alpha=0.3)\n",
    "        plt.plot([0, np.expm1(y_train_log).max()], [0, np.expm1(y_train_log).max()], 'r--', lw=2)\n",
    "        plt.xlabel(\"ì‹¤ì œ ê°’ (ì›)\"); plt.ylabel(\"ì•™ìƒë¸” OOF ì˜ˆì¸¡ ê°’ (ì›)\"); plt.title('ì‹¤ì œ ê°’ vs OOF ì˜ˆì¸¡ ê°’ ë¹„êµ (ì•™ìƒë¸”)', fontsize=16)\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '01_Actual_vs_OOF_Scatter.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> 8.1. ì‹¤ì œ ê°’ vs OOF ì˜ˆì¸¡ ê°’ ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.1(ì‹¤ì œê°’vsì˜ˆì¸¡ê°’) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 2. ì”ì°¨ ë¶„í¬ (ì•™ìƒë¸” ê¸°ì¤€)\n",
    "    try:\n",
    "        logger.write(\">> 8.2. ì”ì°¨ ë¶„í¬ í™•ì¸ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        plt.figure(figsize=(10, 6)); sns.histplot(residuals, kde=True, bins=50)\n",
    "        plt.title('ì”ì°¨(ì‹¤ì œ-ì˜ˆì¸¡) ë¶„í¬ (ì•™ìƒë¸” OOF ê¸°ë°˜)', fontsize=16); plt.xlabel(\"ì”ì°¨ (ì›)\")\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '02_Residuals_Distribution.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> 8.2. ì”ì°¨ ë¶„í¬ í™•ì¸ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.2(ì”ì°¨ ë¶„í¬) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 3. ì”ì°¨ vs ì˜ˆì¸¡ê°’ í”Œë¡¯\n",
    "    try:\n",
    "        logger.write(\">> 8.3. ì”ì°¨ vs ì˜ˆì¸¡ê°’ í”Œë¡¯ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        plt.figure(figsize=(10, 6)); sns.scatterplot(x=np.expm1(ensembled_oof_preds), y=residuals, alpha=0.3)\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.xlabel(\"ì•™ìƒë¸” OOF ì˜ˆì¸¡ ê°’ (ì›)\"); plt.ylabel(\"ì”ì°¨ (ì›)\"); plt.title('ì”ì°¨ vs ì˜ˆì¸¡ê°’ í”Œë¡¯ (ì•™ìƒë¸” OOF ê¸°ë°˜)', fontsize=16)\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '03_Residuals_vs_Predicted.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> 8.3. ì”ì°¨ vs ì˜ˆì¸¡ê°’ í”Œë¡¯ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.3(ì”ì°¨vsì˜ˆì¸¡ê°’) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "        \n",
    "    # 4. ì˜ˆì¸¡ ë¶„í¬ ë¹„êµ\n",
    "    try:\n",
    "        logger.write(\">> 8.4. OOF ì˜ˆì¸¡ê³¼ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì˜ ë¶„í¬ ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        plt.figure(figsize=(10, 6)); sns.kdeplot(np.expm1(ensembled_oof_preds), label='ì•™ìƒë¸” OOF ì˜ˆì¸¡ ê°’', fill=True)\n",
    "        sns.kdeplot(final_predictions, label='ìµœì¢… í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ ê°’', fill=True)\n",
    "        plt.title('OOF ì˜ˆì¸¡ê³¼ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì˜ ë¶„í¬ ë¹„êµ', fontsize=16); plt.xlabel(\"ì˜ˆì¸¡ ê°’ (ì›)\"); plt.legend()\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '04_Prediction_Distribution_Comparison.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> 8.4. OOF ì˜ˆì¸¡ê³¼ í…ŒìŠ¤íŠ¸ ì˜ˆì¸¡ì˜ ë¶„í¬ ë¹„êµ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.4(ì˜ˆì¸¡ ë¶„í¬) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 5. í”¼ì²˜ ì¤‘ìš”ë„ (LGBM ê¸°ì¤€)\n",
    "    try:\n",
    "        logger.write(\">> 8.5. í”¼ì²˜ ì¤‘ìš”ë„ ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "        plt.figure(figsize=(12, 10)); sns.barplot(x=mean_importances.head(20).values, y=mean_importances.head(20).index)\n",
    "        plt.title('ìƒìœ„ 20ê°œ í”¼ì²˜ ì¤‘ìš”ë„ (LGBM í‰ê· )', fontsize=16)\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '05_Feature_Importance.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> 8.5. í”¼ì²˜ ì¤‘ìš”ë„ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.5(í”¼ì²˜ ì¤‘ìš”ë„) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    # 6. SHAP ìš”ì•½ í”Œë¡¯ (LGBM ê¸°ì¤€)\n",
    "    try:\n",
    "        logger.write(\">> 8.6. SHAP ìš”ì•½ í”Œë¡¯ ë¶„ì„ ë° ì €ì¥ ì¤‘...\")\n",
    "        plt.figure()\n",
    "        shap.summary_plot(shap_values, shap_sample, show=False)\n",
    "        plt.title(\"SHAP ìš”ì•½ í”Œë¡¯ (LGBM ë§ˆì§€ë§‰ í´ë“œ ëª¨ë¸)\", fontsize=16)\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '06_SHAP_Summary_Plot.png'), bbox_inches='tight'); plt.close()\n",
    "        logger.write(\">> 8.6. SHAP ìš”ì•½ í”Œë¡¯ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.6(SHAP ìš”ì•½) ë¶„ì„ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "        \n",
    "    # # 7. SHAP ì˜ì¡´ì„± í”Œë¡¯\n",
    "    try:\n",
    "        logger.write(\">> 8.7. SHAP ì˜ì¡´ì„± í”Œë¡¯ ì €ì¥ ì¤‘...\")\n",
    "        top_3_features = mean_importances.head(3).index\n",
    "        \n",
    "        for feature in top_3_features:\n",
    "            # 'ë²•ì •ë™' í”¼ì²˜ì¼ ê²½ìš°ì—ë§Œ í•„í„°ë§ ë¡œì§ì„ ì ìš©í•©ë‹ˆë‹¤.\n",
    "            if feature == 'ë²•ì •ë™':\n",
    "                # 1. 'ë²•ì •ë™' í”¼ì²˜ê°€ shap_sample ë°ì´í„°í”„ë ˆì„ì—ì„œ ëª‡ ë²ˆì§¸ ì—´ì¸ì§€ ì¸ë±ìŠ¤ë¥¼ ì°¾ìŠµë‹ˆë‹¤.\n",
    "                feature_idx = list(shap_sample.columns).index(feature)\n",
    "                \n",
    "                # 2. ê° ë²•ì •ë™ ì½”ë“œë³„ë¡œ í‰ê·  SHAP ê°’ì˜ ì ˆëŒ“ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\n",
    "                #    (ì–´ë–¤ ë™ì´ ê°€ê²©ì— ê°€ì¥ í° ì˜í–¥ì„ ì£¼ëŠ”ì§€ íŒŒì•…í•˜ê¸° ìœ„í•¨)\n",
    "                bjdong_shap_means = {}\n",
    "                unique_bjdongs = shap_sample[feature].unique()\n",
    "                for bjdong_code in unique_bjdongs:\n",
    "                    mask = (shap_sample[feature] == bjdong_code)\n",
    "                    mean_shap_value = np.abs(shap_values[mask, feature_idx]).mean()\n",
    "                    bjdong_shap_means[bjdong_code] = mean_shap_value\n",
    "                \n",
    "                # 3. ê³„ì‚°ëœ SHAP ê°’ì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ìƒìœ„ 20ê°œì˜ ë²•ì •ë™ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
    "                #    [:20] ì´ ìˆ«ìë¥¼ ë°”ê¾¸ë©´ ê°œìˆ˜ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: :30)\n",
    "                top_20_bjdongs = sorted(bjdong_shap_means, key=bjdong_shap_means.get, reverse=True)[:20]\n",
    "                \n",
    "                # 4. ì›ë³¸ ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ìƒìœ„ 20ê°œ ë²•ì •ë™ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„°ë§Œ í•„í„°ë§í•©ë‹ˆë‹¤.\n",
    "                top_20_mask = shap_sample[feature].isin(top_20_bjdongs)\n",
    "                \n",
    "                # 5. í•„í„°ë§ëœ ë°ì´í„°(shap_valuesì™€ shap_sample)ë¡œ ì˜ì¡´ì„± í”Œë¡¯ì„ ìƒì„±í•©ë‹ˆë‹¤.\n",
    "                shap.dependence_plot(\n",
    "                    ind=feature, \n",
    "                    shap_values=shap_values[top_20_mask], \n",
    "                    features=shap_sample[top_20_mask], \n",
    "                    interaction_index=\"auto\", \n",
    "                    show=False\n",
    "                )\n",
    "                \n",
    "                # 6. í”Œë¡¯ì„ ë³´ê¸° ì¢‹ê²Œ ê¾¸ë¯¸ê³  ì €ì¥í•©ë‹ˆë‹¤.\n",
    "                plt.title(f\"SHAP Dependence Plot for '{feature}' (Top 20)\", fontsize=16)\n",
    "                plt.xticks(rotation=45, ha='right') # ë¼ë²¨ì´ ê¸¸ ìˆ˜ ìˆìœ¼ë‹ˆ 45ë„ íšŒì „\n",
    "                plt.tight_layout()\n",
    "                plt.savefig(os.path.join(IMAGE_DIR, f'07_SHAP_Dependence_{feature}_Top20.png'), bbox_inches='tight')\n",
    "                plt.close()\n",
    "\n",
    "            else: # 'ë²•ì •ë™'ì´ ì•„ë‹Œ ë‹¤ë¥¸ í”¼ì²˜(ê±´ë¬¼ë‚˜ì´ ë“±)ëŠ” ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ ì „ì²´ í”Œë¡¯ì„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "                shap.dependence_plot(feature, shap_values, shap_sample, interaction_index=\"auto\", show=False)\n",
    "                plt.title(f\"SHAP Dependence Plot for '{feature}'\", fontsize=16)\n",
    "                plt.savefig(os.path.join(IMAGE_DIR, f'07_SHAP_Dependence_{feature}.png'), bbox_inches='tight')\n",
    "                plt.close()\n",
    "                \n",
    "        logger.write(\">> 8.7. SHAP ì˜ì¡´ì„± í”Œë¡¯ ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.7(SHAP ì˜ì¡´ì„±) ì‹œê°í™” ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "        \n",
    "    # 8. í•™ìŠµ ê³¡ì„ \n",
    "    try:\n",
    "        logger.write(\">> 8.8. í•™ìŠµ ê³¡ì„  ì´ë¯¸ì§€ ì €ì¥ ì¤‘...\")\n",
    "\n",
    "        # LGBMì´ í•™ìŠµí–ˆë˜ 'category' íƒ€ì…ì˜ ë°ì´í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
    "        train_sizes, train_scores, validation_scores = learning_curve(\n",
    "            estimator=lgb.LGBMRegressor(**best_params, verbosity=-1, random_state=Config.SEED, device='cuda'),\n",
    "            X=X_train_lgbm, # X_train -> X_train_lgbm ìœ¼ë¡œ ìˆ˜ì •\n",
    "            y=y_train_log, \n",
    "            train_sizes=np.linspace(0.1, 1.0, 5),\n",
    "            cv=TimeSeriesSplit(n_splits=3), \n",
    "            scoring='neg_root_mean_squared_error', \n",
    "            n_jobs=-1)\n",
    "        \n",
    "        train_scores_mean = -train_scores.mean(axis=1)\n",
    "        validation_scores_mean = -validation_scores.mean(axis=1)\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "        plt.plot(train_sizes, validation_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "        plt.title(\"í•™ìŠµ ê³¡ì„  (Learning Curve)\", fontsize=16)\n",
    "        plt.xlabel(\"í•™ìŠµ ë°ì´í„° ìƒ˜í”Œ ìˆ˜\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.legend(loc=\"best\")\n",
    "        plt.grid()\n",
    "        plt.savefig(os.path.join(IMAGE_DIR, '08_learning_curve.png'), bbox_inches='tight')\n",
    "        plt.close()\n",
    "        logger.write(\">> 8.8. í•™ìŠµ ê³¡ì„  ì´ë¯¸ì§€ ì €ì¥ ì™„ë£Œ.\")\n",
    "    except Exception as e:\n",
    "        logger.write(f\">> [ì˜¤ë¥˜] 8.8(í•™ìŠµ ê³¡ì„ ) ìƒì„± ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "\n",
    "    logger.write(\">> [8ë‹¨ê³„ ì™„ë£Œ] ëª¨ë“  ì‹œê°í™” ë° ë¶„ì„ íŒŒì¼ ì €ì¥ ì™„ë£Œ.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logger.write(f\">> [ì˜¤ë¥˜] 8ë‹¨ê³„(ì‹œê°í™” ë° ë¶„ì„) ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì¤‘ ë¬¸ì œ ë°œìƒ: {e}\", print_error=True)\n",
    "finally:\n",
    "    logger.write(\"\\n\" + \"=\"*60)\n",
    "    logger.write(\"ğŸ‰ ëª¨ë“  í”„ë¡œì„¸ìŠ¤ê°€ ì„±ê³µì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    logger.write(\"=\"*60)\n",
    "    # logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "house_price_predict_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
